{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q nltk\n",
    "%pip install -U -q gdown\n",
    "%pip install -U -q scikit-learn\n",
    "%pip install -U -q setuptools wheel\n",
    "%pip install -U -q spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import gdown\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\crane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crane\\OneDrive\\Documenten\\Desktop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if URL does not work please download the file from the link and upload it to the current directory\n",
    "#  Or use that download link and change the url in the code below\n",
    "# https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?select=twitter_training.csvhttps://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?select=twitter_training.csv\n",
    "\n",
    "training_url = \"https://storage.googleapis.com/kaggle-data-sets/1520310/2510329/compressed/twitter_training.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240218%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240218T155436Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=82193761d8a7d133a6be4580fad6d9f48e4f8a275745ca6b3a127169aa7bd9e3972778091043f7d8f12fb022481de0b4abcc14fb2ff0cb120e7f81225cf7ed24287f9e227eaa420a2432d4b7712b8201b6eeceef2329a832bb2c33a2aff854599db8de393ddb8e6654c13ae186231e75a346a1f0ba3cd07b3f9cf00eb79a312faacd3e941417d1dfafc7a5b86d90fd3366c18c39c5fd469b1336f061cbf53743985acfb6926f102be2aef2194d81bc0dee7b2a5e8ba6b9caf0ef381ae100bd5e31d0419ad2ccd6932843dbd667535137464d1f45090d59946103eb9a3b7cfaa19e3fe35c33fe4fc6b9c030d923953e952bb5d21c3b07c45cfe3c067cffd90131\"\n",
    "test_url = \"https://storage.googleapis.com/kagglesdsdata/datasets/1520310/2510329/twitter_validation.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240218%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240218T155644Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=87e33c9d2930ac1781d28b2ba7585933c4439487cf0dab529fab45f8a9af4edacc6de4a06ad747c8d7c9c94645980bd1cdf255b3882793e1dc7f3472da08880e2e293ce2b8b551e22c5b4a5493a228ec086c79e78f5fd8245cf530fa7f4e77fb86cf398cc8ef4dabdfa6a00f39c9d7f9577a5afa9b9f2bbc609cc896b252018dac65a40fe10503e074bc4aed03f27ed7632975d01dd01492b6ddea59e9f7cc435b805221a9b710a76d336e312e8ee777eb6009780aad198e9fcd406ea9de3d0797fb86d010b82fa8b4a1b0f0381a7a8d188ece2a6352a47746fa90cb2e358e822bdb7ba64b8519356d1cc40311183b91d1bd25a952ca960bc6e7d5f277c3a905\"\n",
    "gdown.download(training_url, \"twitter_training.csv.zip\", quiet=False)\n",
    "gdown.download(test_url, \"twitter_validation.csv\", quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\crane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "c:\\Users\\crane\\anaconda3\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "#load lexicon for sentiment analysis and spacy model\n",
    "nltk.download('vader_lexicon')\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\crane\\\\OneDrive\\\\Documents\\\\Desktop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"twitter_training.csv\", names=[\"ID\", \"Entity\",\"Sentiment\",\"Text\"], header=None)\n",
    "val_data = pd.read_csv(\"twitter_validation.csv\", names=[\"ID\", \"Entity\",\"Sentiment\",\"Text\"], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID               Entity   Sentiment  \\\n",
       "0    3364             Facebook  Irrelevant   \n",
       "1     352               Amazon     Neutral   \n",
       "2    8312            Microsoft    Negative   \n",
       "3    4371                CS-GO    Negative   \n",
       "4    4433               Google     Neutral   \n",
       "..    ...                  ...         ...   \n",
       "995  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "996  4359                CS-GO  Irrelevant   \n",
       "997  2652          Borderlands    Positive   \n",
       "998  8069            Microsoft    Positive   \n",
       "999  6960      johnson&johnson     Neutral   \n",
       "\n",
       "                                                  Text  \n",
       "0    I mentioned on Facebook that I was struggling ...  \n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       "2    @Microsoft Why do I pay for WORD when it funct...  \n",
       "3    CSGO matchmaking is so full of closet hacking,...  \n",
       "4    Now the President is slapping Americans in the...  \n",
       "..                                                 ...  \n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  \n",
       "997  Today sucked so it’s time to drink wine n play...  \n",
       "998  Bought a fraction of Microsoft today. Small wins.  \n",
       "999  Johnson & Johnson to stop selling talc baby po...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Cell with 10 head of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=train_data.head(10).Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = [\"ADJ\", \"NOUN\", \"VERB\", \"ADV\", \"PART\", \"CCONJ\"]\n",
    "\n",
    "def run_vader(tmp, pos_tags=pos_tags, lemmatize=False, part_of_speech=False, verbose=False):\n",
    "    vader_output=[]\n",
    "    for item in tmp:\n",
    "        input_text = []\n",
    "        # tokenize the text, add pos tags and dependency parsing\n",
    "        sent=nlp(str(item))\n",
    "        for token in sent:\n",
    "            text = token.text\n",
    "            if lemmatize:\n",
    "                text = token.lemma_\n",
    "                if text == \"-PRON-\":\n",
    "                    text = token.text\n",
    "            if part_of_speech:\n",
    "                if token.pos_ in pos_tags:\n",
    "                    input_text.append(text)\n",
    "            else:\n",
    "                input_text.append(text)\n",
    "                \n",
    "        score=vader_model.polarity_scores(' '.join(input_text))\n",
    "        if verbose:\n",
    "            print(sent)\n",
    "            print(input_text)\n",
    "            print(score)\n",
    "            print(\"\\n\")\n",
    "        vader_output.append(score)\n",
    "    return vader_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('not', 'PART'), ('a', 'DET'), ('sentence', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('I', 'PRON'), ('hope', 'VERB'), ('you', 'PRON'), ('enjoy', 'VERB'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"This is not a sentence, but I hope you enjoy.\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=run_vader(tmp, pos_tags=pos_tags, lemmatize=True, part_of_speech=True, verbose=False)\n",
    "def vader_score(output):\n",
    "    pred = []\t\n",
    "    for item in output:\n",
    "        if item['compound'] > 0.35:\n",
    "            pred.append(\"Positive\")\n",
    "        elif item['compound'] < -0.35:\n",
    "            pred.append(\"Negative\")\n",
    "        else:\n",
    "            pred.append(\"Neutral\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive',\n",
       " 'Positive']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_score(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Cleaning*\n",
    "\n",
    "\n",
    "drop Irrelevant values\n",
    "\n",
    "balance data\n",
    "\n",
    "shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID       Entity Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "74681  9200       Nvidia  Positive   \n",
       "\n",
       "                                                    Text  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      I am coming to the borders and I will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "74677  Just realized that the Windows partition of my...  \n",
       "74678  Just realized that my Mac window partition is ...  \n",
       "74679  Just realized the windows partition of my Mac ...  \n",
       "74680  Just realized between the windows partition of...  \n",
       "74681  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74682 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[train_data.Sentiment != \"Irrelevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    22542\n",
       "Positive    20832\n",
       "Neutral     18318\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "Entity         0\n",
       "Sentiment      0\n",
       "Text         571\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crane\\AppData\\Local\\Temp\\ipykernel_26544\\1654660196.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Entity       0\n",
       "Sentiment    0\n",
       "Text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(data):\n",
    "    pos_data = data[data.Sentiment == \"Positive\"]\n",
    "    neg_data = data[data.Sentiment == \"Negative\"]\n",
    "    neu_data = data[data.Sentiment == \"Neutral\"]\n",
    "    minimum=min(len(pos_data), len(neg_data), len(neu_data))\n",
    "    pos_data = pos_data.sample(n=minimum, replace=True, random_state=123)\n",
    "    neg_data = neg_data.sample(n=minimum, replace=True, random_state=123)\n",
    "    neu_data = neu_data.sample(n=minimum, replace=True, random_state=123)\n",
    "    new_df = pd.concat([pos_data, neg_data, neu_data])\n",
    "    new_df = new_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = balancing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    18108\n",
       "Positive    18108\n",
       "Neutral     18108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon-Base Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=run_vader(train_data.Text, pos_tags=pos_tags, lemmatize=True, part_of_speech=True, verbose=False)\n",
    "pred = vader_score(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.58      0.43      0.49     18108\n",
      "     Neutral       0.38      0.49      0.43     18108\n",
      "    Positive       0.55      0.54      0.55     18108\n",
      "\n",
      "    accuracy                           0.49     54324\n",
      "   macro avg       0.51      0.49      0.49     54324\n",
      "weighted avg       0.51      0.49      0.49     54324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = sklearn.metrics.classification_report(y_true=train_data.Sentiment, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.58      0.46      0.51       266\n",
      "     Neutral       0.38      0.44      0.41       266\n",
      "    Positive       0.60      0.62      0.61       266\n",
      "\n",
      "    accuracy                           0.51       798\n",
      "   macro avg       0.52      0.51      0.51       798\n",
      "weighted avg       0.52      0.51      0.51       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output=run_vader(val_data.Text, pos_tags=pos_tags, lemmatize=True, part_of_speech=True, verbose=False)\n",
    "pred = vader_score(output)\n",
    "report = sklearn.metrics.classification_report(y_true=val_data.Sentiment, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(file, vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    with open(file, \"w\") as f:\n",
    "        f.write(\"Important words in negative documents\\n\")\n",
    "        for coef, feat in topn_class1:\n",
    "            f.write(f\"{class_labels[0]} {coef} {feat}\\n\")\n",
    "        f.write(\"-----------------------------------------\\n\")\n",
    "        f.write(\"Important words in neutral documents\\n\")\n",
    "        for coef, feat in topn_class2:\n",
    "            f.write(f\"{class_labels[1]} {coef} {feat}\\n\")\n",
    "        f.write(\"-----------------------------------------\\n\")\n",
    "        f.write(\"Important words in positive documents\\n\")\n",
    "        for coef, feat in topn_class3:\n",
    "            f.write(f\"{class_labels[2]} {coef} {feat}\\n\")\n",
    "        f.close()\n",
    "        \n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinoial Naive Bayes Classifier using CountVectorizer and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crane\\AppData\\Local\\Temp\\ipykernel_26544\\2375746848.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_data.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "val_data = val_data[val_data.Sentiment != \"Irrelevant\"]\n",
    "val_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = balancing(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    266\n",
       "Neutral     266\n",
       "Negative    266\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.Sentiment.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    token = []\n",
    "    sent = nlp(str(text))\n",
    "    for word in sent:\n",
    "        if word.is_stop == False:\n",
    "            if len(word.lemma_) > 2:\n",
    "                token.append(word.lemma_)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crane\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=20, \n",
    "    max_df=0.7,\n",
    "    tokenizer=tokenization, \n",
    "    stop_words='english'\n",
    ")\n",
    "input=count_vectorizer.fit_transform(train_data.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.79      0.74       266\n",
      "     Neutral       0.78      0.61      0.68       266\n",
      "    Positive       0.70      0.77      0.74       266\n",
      "\n",
      "    accuracy                           0.72       798\n",
      "   macro avg       0.73      0.72      0.72       798\n",
      "weighted avg       0.73      0.72      0.72       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(input, train_data.Sentiment)\n",
    "pred = model.predict(count_vectorizer.transform(val_data.Text))\t\n",
    "report = metrics.classification_report(y_true=val_data.Sentiment, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\crane\\\\OneDrive\\\\Documenten\\\\Desktop'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "Negative 3666.0 game\n",
      "Negative 2210.0 ...\n",
      "Negative 1651.0 play\n",
      "Negative 1380.0 like\n",
      "Negative 1357.0 fuck\n",
      "Negative 1286.0 shit\n",
      "Negative 1161.0 fix\n",
      "Negative 1033.0 bad\n",
      "Negative 946.0 time\n",
      "Negative 794.0 people\n",
      "Negative 757.0 fifa\n",
      "Negative 724.0 year\n",
      "Negative 681.0 work\n",
      "Negative 669.0 johnson\n",
      "Negative 661.0 unk\n",
      "Negative 644.0 try\n",
      "Negative 643.0 good\n",
      "Negative 641.0 fucking\n",
      "Negative 629.0 server\n",
      "Negative 619.0 new\n",
      "Negative 602.0 know\n",
      "Negative 599.0 look\n",
      "Negative 587.0 day\n",
      "Negative 581.0 @eamaddennfl\n",
      "Negative 569.0 facebook\n",
      "Negative 552.0 guy\n",
      "Negative 544.0 update\n",
      "Negative 541.0 want\n",
      "Negative 540.0 rhandlerr\n",
      "Negative 529.0 verizon\n",
      "Negative 521.0 think\n",
      "Negative 517.0 help\n",
      "Negative 499.0 need\n",
      "Negative 499.0 buy\n",
      "Negative 495.0 home\n",
      "Negative 491.0 problem\n",
      "Negative 490.0 damn\n",
      "Negative 476.0 @nba2k\n",
      "Negative 470.0 player\n",
      "Negative 451.0 microsoft\n",
      "Negative 448.0 wtf\n",
      "Negative 440.0 ban\n",
      "Negative 431.0 stop\n",
      "Negative 420.0 xbox\n",
      "Negative 420.0 say\n",
      "Negative 418.0 @rainbow6game\n",
      "Negative 403.0 right\n",
      "Negative 395.0 google\n",
      "Negative 393.0 ass\n",
      "Negative 389.0 thing\n",
      "Negative 386.0 come\n",
      "Negative 384.0 man\n",
      "Negative 383.0 make\n",
      "Negative 381.0 gta\n",
      "Negative 379.0 money\n",
      "Negative 379.0 break\n",
      "Negative 377.0 pubg\n",
      "Negative 376.0 hate\n",
      "Negative 363.0 start\n",
      "Negative 363.0 depot\n",
      "Negative 363.0 amazon\n",
      "Negative 356.0 account\n",
      "Negative 349.0 service\n",
      "Negative 349.0 company\n",
      "Negative 346.0 pay\n",
      "Negative 346.0 dead\n",
      "Negative 346.0 ....\n",
      "Negative 341.0 lose\n",
      "Negative 337.0 suck\n",
      "Negative 333.0 overwatch\n",
      "Negative 327.0 actually\n",
      "Negative 324.0 team\n",
      "Negative 323.0 league\n",
      "Negative 320.0 duty\n",
      "Negative 318.0 way\n",
      "Negative 312.0 app\n",
      "Negative 308.0 big\n",
      "Negative 305.0 issue\n",
      "Negative 301.0 tell\n",
      "Negative 301.0 fortnite\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "Neutral 4413.0 ...\n",
      "Neutral 1779.0 johnson\n",
      "Neutral 1610.0 game\n",
      "Neutral 1475.0 play\n",
      "Neutral 1196.0 good\n",
      "Neutral 1114.0 amazon\n",
      "Neutral 995.0 like\n",
      "Neutral 921.0 new\n",
      "Neutral 844.0 win\n",
      "Neutral 816.0 facebook\n",
      "Neutral 770.0 google\n",
      "Neutral 729.0 microsoft\n",
      "Neutral 710.0 red\n",
      "Neutral 707.0 dead\n",
      "Neutral 706.0 time\n",
      "Neutral 689.0 unk\n",
      "Neutral 683.0 xbox\n",
      "Neutral 675.0 watch\n",
      "Neutral 675.0 video\n",
      "Neutral 662.0 nvidia\n",
      "Neutral 657.0 people\n",
      "Neutral 647.0 thank\n",
      "Neutral 629.0 look\n",
      "Neutral 625.0 com\n",
      "Neutral 618.0 great\n",
      "Neutral 617.0 love\n",
      "Neutral 610.0 ....\n",
      "Neutral 597.0 come\n",
      "Neutral 574.0 day\n",
      "Neutral 570.0 live\n",
      "Neutral 541.0 try\n",
      "Neutral 509.0 world\n",
      "Neutral 501.0 2020\n",
      "Neutral 495.0 verizon\n",
      "Neutral 484.0 redemption\n",
      "Neutral 472.0 home\n",
      "Neutral 458.0 today\n",
      "Neutral 451.0 legend\n",
      "Neutral 449.0 work\n",
      "Neutral 446.0 check\n",
      "Neutral 439.0 bad\n",
      "Neutral 432.0 know\n",
      "Neutral 429.0 stop\n",
      "Neutral 429.0 news\n",
      "Neutral 427.0 series\n",
      "Neutral 427.0 say\n",
      "Neutral 426.0 want\n",
      "Neutral 410.0 https\n",
      "Neutral 400.0 ps5\n",
      "Neutral 394.0 man\n",
      "Neutral 394.0 borderland\n",
      "Neutral 389.0 rhandlerr\n",
      "Neutral 382.0 free\n",
      "Neutral 379.0 team\n",
      "Neutral 379.0 kill\n",
      "Neutral 379.0 baby\n",
      "Neutral 378.0 year\n",
      "Neutral 375.0 card\n",
      "Neutral 371.0 sell\n",
      "Neutral 370.0 shit\n",
      "Neutral 361.0 buy\n",
      "Neutral 352.0 player\n",
      "Neutral 344.0 warcraft\n",
      "Neutral 336.0 black\n",
      "Neutral 335.0 powder\n",
      "Neutral 331.0 chance\n",
      "Neutral 329.0 think\n",
      "Neutral 329.0 big\n",
      "Neutral 327.0 stream\n",
      "Neutral 325.0 league\n",
      "Neutral 319.0 need\n",
      "Neutral 311.0 hearthstone\n",
      "Neutral 311.0 dota\n",
      "Neutral 309.0 100\n",
      "Neutral 304.0 help\n",
      "Neutral 304.0 fuck\n",
      "Neutral 293.0 start\n",
      "Neutral 291.0 rank\n",
      "Neutral 290.0 feel\n",
      "Neutral 284.0 update\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "Positive 2748.0 game\n",
      "Positive 2421.0 ...\n",
      "Positive 2300.0 good\n",
      "Positive 2152.0 play\n",
      "Positive 1756.0 love\n",
      "Positive 1270.0 like\n",
      "Positive 1079.0 new\n",
      "Positive 1061.0 thank\n",
      "Positive 1027.0 look\n",
      "Positive 961.0 great\n",
      "Positive 924.0 time\n",
      "Positive 829.0 fun\n",
      "Positive 807.0 wait\n",
      "Positive 665.0 come\n",
      "Positive 657.0 unk\n",
      "Positive 656.0 year\n",
      "Positive 635.0 red\n",
      "Positive 628.0 day\n",
      "Positive 616.0 amazing\n",
      "Positive 609.0 dead\n",
      "Positive 607.0 borderland\n",
      "Positive 564.0 creed\n",
      "Positive 557.0 xbox\n",
      "Positive 549.0 assassin\n",
      "Positive 545.0 rhandlerr\n",
      "Positive 521.0 happy\n",
      "Positive 515.0 know\n",
      "Positive 514.0 think\n",
      "Positive 500.0 want\n",
      "Positive 499.0 nice\n",
      "Positive 497.0 work\n",
      "Positive 487.0 excited\n",
      "Positive 484.0 redemption\n",
      "Positive 468.0 people\n",
      "Positive 464.0 today\n",
      "Positive 456.0 gta\n",
      "Positive 444.0 feel\n",
      "Positive 429.0 wow\n",
      "Positive 421.0 buy\n",
      "Positive 420.0 home\n",
      "Positive 410.0 ps5\n",
      "Positive 408.0 awesome\n",
      "Positive 406.0 microsoft\n",
      "Positive 405.0 series\n",
      "Positive 402.0 finally\n",
      "Positive 401.0 stream\n",
      "Positive 394.0 thing\n",
      "Positive 386.0 shit\n",
      "Positive 385.0 ....\n",
      "Positive 379.0 overwatch\n",
      "Positive 379.0 fuck\n",
      "Positive 375.0 legend\n",
      "Positive 370.0 fifa\n",
      "Positive 364.0 world\n",
      "Positive 359.0 win\n",
      "Positive 357.0 watch\n",
      "Positive 354.0 nvidia\n",
      "Positive 343.0 favorite\n",
      "Positive 341.0 guy\n",
      "Positive 339.0 enjoy\n",
      "Positive 335.0 cool\n",
      "Positive 332.0 video\n",
      "Positive 332.0 right\n",
      "Positive 327.0 miss\n",
      "Positive 325.0 actually\n",
      "Positive 317.0 black\n",
      "Positive 313.0 start\n",
      "Positive 308.0 depot\n",
      "Positive 306.0 pubg\n",
      "Positive 304.0 johnson\n",
      "Positive 298.0 bad\n",
      "Positive 296.0 league\n",
      "Positive 293.0 make\n",
      "Positive 293.0 battlefield\n",
      "Positive 287.0 let\n",
      "Positive 284.0 verizon\n",
      "Positive 282.0 help\n",
      "Positive 281.0 team\n",
      "Positive 275.0 big\n",
      "Positive 273.0 war\n"
     ]
    }
   ],
   "source": [
    "important_features_per_class(\"NB_CountVector.txt\", count_vectorizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crane\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.79      0.74       266\n",
      "     Neutral       0.76      0.62      0.68       266\n",
      "    Positive       0.74      0.75      0.74       266\n",
      "\n",
      "    accuracy                           0.72       798\n",
      "   macro avg       0.73      0.72      0.72       798\n",
      "weighted avg       0.73      0.72      0.72       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_transformer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer=tokenization,\n",
    "    stop_words=\"english\",\n",
    "    min_df=20,\n",
    "    max_df=0.7,\n",
    ")\n",
    "model=MultinomialNB()\n",
    "model.fit(tfidf_transformer.fit_transform(train_data.Text), train_data.Sentiment)\n",
    "pred = model.predict(tfidf_transformer.transform(val_data.Text))\n",
    "report = metrics.classification_report(y_true=val_data.Sentiment, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "Negative 603.0764010907648 game\n",
      "Negative 484.7843586223601 fuck\n",
      "Negative 398.8253114029139 shit\n",
      "Negative 344.2967670291982 ...\n",
      "Negative 329.1589885050229 play\n",
      "Negative 303.8763549727962 fix\n",
      "Negative 290.00630074154367 bad\n",
      "Negative 281.19086680103123 like\n",
      "Negative 247.9689154600943 fifa\n",
      "Negative 223.208555018913 wtf\n",
      "Negative 211.57873461770131 time\n",
      "Negative 209.87607422519076 unk\n",
      "Negative 199.80520433921984 server\n",
      "Negative 196.29630915023884 fucking\n",
      "Negative 195.95164044687866 @eamaddennfl\n",
      "Negative 184.21537422459267 damn\n",
      "Negative 172.58234588813477 people\n",
      "Negative 163.5771709572558 @nba2k\n",
      "Negative 163.4332901684593 verizon\n",
      "Negative 163.35554202558956 year\n",
      "Negative 160.0570619199928 facebook\n",
      "Negative 158.1583525837373 work\n",
      "Negative 151.14524456776732 know\n",
      "Negative 147.59009830889934 hate\n",
      "Negative 145.85868765412116 johnson\n",
      "Negative 145.1914297242631 good\n",
      "Negative 144.65254417953773 try\n",
      "Negative 142.3475372110161 rhandlerr\n",
      "Negative 139.56344541766396 update\n",
      "Negative 137.19457372743972 new\n",
      "Negative 137.08486217503398 problem\n",
      "Negative 136.9889068669731 guy\n",
      "Negative 136.69076686099407 look\n",
      "Negative 135.88073826120123 @rainbow6game\n",
      "Negative 134.07311781645916 home\n",
      "Negative 133.60138044256044 ban\n",
      "Negative 133.59890594933864 think\n",
      "Negative 130.36761523954715 ass\n",
      "Negative 130.19852766987424 day\n",
      "Negative 127.2832315275468 stop\n",
      "Negative 126.1224030164353 want\n",
      "Negative 126.02392317008962 man\n",
      "Negative 123.04778414352553 need\n",
      "Negative 122.6966017322943 suck\n",
      "Negative 121.97209462724012 buy\n",
      "Negative 117.57082641147689 player\n",
      "Negative 116.85089876175525 gta\n",
      "Negative 115.66534782764482 help\n",
      "Negative 115.18944538567274 microsoft\n",
      "Negative 115.10909208049422 fortnite\n",
      "Negative 114.00427303801594 say\n",
      "Negative 113.95862287608689 depot\n",
      "Negative 112.7655489071499 money\n",
      "Negative 112.46917899220792 break\n",
      "Negative 112.39137470995855 pubg\n",
      "Negative 110.91811001430733 xbox\n",
      "Negative 109.17463125763014 overwatch\n",
      "Negative 107.5903695335525 eamaddennfl\n",
      "Negative 107.18246692192885 thing\n",
      "Negative 107.11433188580655 google\n",
      "Negative 105.66429410829225 league\n",
      "Negative 105.43797040997259 hell\n",
      "Negative 104.31460585427224 right\n",
      "Negative 103.57966866108089 make\n",
      "Negative 101.36807569989416 rainbow6game\n",
      "Negative 97.93287268018629 duty\n",
      "Negative 95.70688550656251 dead\n",
      "Negative 95.67517111099458 start\n",
      "Negative 94.98037798082618 lose\n",
      "Negative 93.16052929002649 stupid\n",
      "Negative 92.22160573443436 trash\n",
      "Negative 91.5722506376625 company\n",
      "Negative 91.05309570183287 account\n",
      "Negative 89.17266489874964 ....\n",
      "Negative 88.747253114292 amazon\n",
      "Negative 88.21103746797523 pic.twitter.com\n",
      "Negative 87.29158143354319 way\n",
      "Negative 86.5228516375504 legend\n",
      "Negative 85.78996724215638 tell\n",
      "Negative 85.2550802464406 come\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "Neutral 710.9979072212611 ...\n",
      "Neutral 389.42085250019113 johnson\n",
      "Neutral 264.2776527632163 game\n",
      "Neutral 262.0208782178321 play\n",
      "Neutral 242.25985637536832 amazon\n",
      "Neutral 235.52285143571171 good\n",
      "Neutral 231.95256550788707 red\n",
      "Neutral 226.92743440124778 dead\n",
      "Neutral 217.74503397876998 unk\n",
      "Neutral 216.6507241365538 video\n",
      "Neutral 203.90171216094564 new\n",
      "Neutral 203.47970167746152 like\n",
      "Neutral 201.33256805828324 facebook\n",
      "Neutral 195.66075284863143 check\n",
      "Neutral 187.70446927314083 win\n",
      "Neutral 186.6123966161805 google\n",
      "Neutral 181.16814445078955 watch\n",
      "Neutral 178.919984894626 microsoft\n",
      "Neutral 176.07164533440235 redemption\n",
      "Neutral 173.68847175366662 live\n",
      "Neutral 165.1266984706578 xbox\n",
      "Neutral 162.9360842806432 look\n",
      "Neutral 159.52768639530012 time\n",
      "Neutral 157.98045815948672 love\n",
      "Neutral 155.91627635840564 nvidia\n",
      "Neutral 155.87899331472772 ....\n",
      "Neutral 154.5485123670403 great\n",
      "Neutral 148.85872717389105 com\n",
      "Neutral 148.1802163289108 come\n",
      "Neutral 146.45397991462173 thank\n",
      "Neutral 138.6127117807493 day\n",
      "Neutral 131.95561351618457 world\n",
      "Neutral 130.9274759662326 people\n",
      "Neutral 130.87220437523408 verizon\n",
      "Neutral 130.47659984891945 2020\n",
      "Neutral 127.95476679684735 legend\n",
      "Neutral 127.64429869615891 try\n",
      "Neutral 124.32644796306874 kill\n",
      "Neutral 121.77291031987188 earn\n",
      "Neutral 120.87366447269252 borderland\n",
      "Neutral 118.99322718972319 rank\n",
      "Neutral 116.3572507675715 achievement\n",
      "Neutral 116.29285762621188 bad\n",
      "Neutral 113.39820184036451 series\n",
      "Neutral 113.10792437140478 man\n",
      "Neutral 112.0066025136671 https\n",
      "Neutral 111.92656458407373 stop\n",
      "Neutral 111.89196554964917 ps5\n",
      "Neutral 111.42421840575479 say\n",
      "Neutral 105.94838804810794 today\n",
      "Neutral 105.0866614737829 know\n",
      "Neutral 104.96337493135215 work\n",
      "Neutral 104.20663996197649 news\n",
      "Neutral 102.46805947277268 home\n",
      "Neutral 101.04655705140873 warcraft\n",
      "Neutral 99.64089955138054 dota\n",
      "Neutral 97.68447341852945 league\n",
      "Neutral 97.61423929208962 baby\n",
      "Neutral 95.72581714492824 want\n",
      "Neutral 94.25257385925347 sell\n",
      "Neutral 93.62791362364167 shit\n",
      "Neutral 92.18438832053317 hearthstone\n",
      "Neutral 90.35808972039628 free\n",
      "Neutral 89.29859328115099 player\n",
      "Neutral 89.00262556430933 buy\n",
      "Neutral 88.12460669026974 stream\n",
      "Neutral 87.19565915252001 powder\n",
      "Neutral 87.06920126577278 team\n",
      "Neutral 85.7577784045663 rhandlerr\n",
      "Neutral 85.62374551218421 big\n",
      "Neutral 83.47198964300497 year\n",
      "Neutral 83.17323062364645 card\n",
      "Neutral 82.7647380054399 black\n",
      "Neutral 82.5588871912827 chance\n",
      "Neutral 82.17651041199882 think\n",
      "Neutral 81.78481055528715 apex\n",
      "Neutral 81.15156649225499 happy\n",
      "Neutral 78.25103824363737 csgo\n",
      "Neutral 77.86443828562511 .....\n",
      "Neutral 76.65369577142285 t.co\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "Positive 637.992118669873 good\n",
      "Positive 551.4565197779829 love\n",
      "Positive 494.96592387086747 game\n",
      "Positive 451.13718559142126 play\n",
      "Positive 420.8199195836018 ...\n",
      "Positive 333.40632423806596 thank\n",
      "Positive 305.98725721588886 wait\n",
      "Positive 298.8926780835651 look\n",
      "Positive 287.66193567970845 like\n",
      "Positive 284.23281745545387 fun\n",
      "Positive 277.8230833614788 great\n",
      "Positive 261.47561219666557 new\n",
      "Positive 259.62109868002347 wow\n",
      "Positive 249.66282241978655 unk\n",
      "Positive 233.60263687764137 time\n",
      "Positive 231.05649005112406 amazing\n",
      "Positive 225.78012167714712 nice\n",
      "Positive 185.85409660205903 borderland\n",
      "Positive 182.53213512278813 happy\n",
      "Positive 179.01560329146753 excited\n",
      "Positive 174.43828292519544 red\n",
      "Positive 174.38954567005928 creed\n",
      "Positive 174.08707238703042 day\n",
      "Positive 173.37445011241877 assassin\n",
      "Positive 169.07072456606872 dead\n",
      "Positive 167.86891333518807 come\n",
      "Positive 162.9849939586455 year\n",
      "Positive 155.55379224087216 gta\n",
      "Positive 155.0817513439862 xbox\n",
      "Positive 152.85620396920734 awesome\n",
      "Positive 147.92280174555083 cool\n",
      "Positive 145.88865383310633 want\n",
      "Positive 145.2984666700663 think\n",
      "Positive 143.27389679858956 miss\n",
      "Positive 140.93430193572905 know\n",
      "Positive 140.87448277788044 redemption\n",
      "Positive 139.31310102968885 rhandlerr\n",
      "Positive 136.16858910738262 ps5\n",
      "Positive 136.06037830672503 beautiful\n",
      "Positive 131.9252191239609 overwatch\n",
      "Positive 130.5480466604801 work\n",
      "Positive 128.54197806031493 feel\n",
      "Positive 127.92242047282515 buy\n",
      "Positive 122.0155648509079 shit\n",
      "Positive 121.78952174586213 stream\n",
      "Positive 120.3586624264142 fuck\n",
      "Positive 118.72797338893746 finally\n",
      "Positive 113.64313699834585 enjoy\n",
      "Positive 113.40991778174872 fifa\n",
      "Positive 113.39375919502146 favorite\n",
      "Positive 113.31065082797058 thing\n",
      "Positive 112.7789270820073 home\n",
      "Positive 110.60446179162628 ....\n",
      "Positive 110.04468269881556 series\n",
      "Positive 110.04137291017825 battlefield\n",
      "Positive 109.95362590471177 today\n",
      "Positive 106.3102396234358 let\n",
      "Positive 106.0506295311721 win\n",
      "Positive 105.4746947027349 microsoft\n",
      "Positive 105.10157599923407 pretty\n",
      "Positive 104.63079458010618 guy\n",
      "Positive 103.20602719082315 legend\n",
      "Positive 101.62129907458832 nvidia\n",
      "Positive 101.57117850707515 right\n",
      "Positive 100.07104858491549 actually\n",
      "Positive 99.14922767047123 world\n",
      "Positive 98.19165454825841 watch\n",
      "Positive 96.57622733086706 start\n",
      "Positive 92.76520204571814 man\n",
      "Positive 92.28308232728449 damn\n",
      "Positive 91.98057133356554 people\n",
      "Positive 91.58597940655498 depot\n",
      "Positive 91.4195127165159 league\n",
      "Positive 90.44611775227918 pubg\n",
      "Positive 89.39609666144743 god\n",
      "Positive 89.18550631506214 fortnite\n",
      "Positive 88.12432366694891 video\n",
      "Positive 87.98463687300578 pic.twitter.com\n",
      "Positive 87.94561801593937 interesting\n",
      "Positive 87.17667840545548 hearthstone\n"
     ]
    }
   ],
   "source": [
    "important_features_per_class(\"NB_TFIDF.txt\", tfidf_transformer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
